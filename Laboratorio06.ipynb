{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Laboratorio06.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FsrLXj2CyH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "# Install TensorFlow\n",
        "try:\n",
        "  # %tensorflow_version only works in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-71JfamubbF",
        "colab_type": "code",
        "outputId": "a4d0ca40-d535-44b6-8e04-7ed34979b182",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhB_N3oVCzqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cnn model for the har dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJqkUOWSJmba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_path = \"/content/drive/My Drive/SP1_2020/HAR with CNN/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5O6cyb5PGFIO",
        "colab_type": "code",
        "outputId": "d4e76229-fba2-49f2-8d41-0947ad7c801c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "# run this only once to save dataset to drive\n",
        "!wget https://raw.githubusercontent.com/jbrownlee/Datasets/master/HAR_Smartphones.zip -P \"/content/drive/My Drive/SP1_2020/HAR with CNN/\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-29 17:27:32--  https://raw.githubusercontent.com/jbrownlee/Datasets/master/HAR_Smartphones.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60999314 (58M) [application/zip]\n",
            "Saving to: ‘/content/drive/My Drive/SP1_2020/HAR with CNN/HAR_Smartphones.zip’\n",
            "\n",
            "HAR_Smartphones.zip 100%[===================>]  58.17M  41.6MB/s    in 1.4s    \n",
            "\n",
            "2020-02-29 17:27:34 (41.6 MB/s) - ‘/content/drive/My Drive/SP1_2020/HAR with CNN/HAR_Smartphones.zip’ saved [60999314/60999314]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1jj5h-UHN5s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/drive/My Drive/SP1_2020/HAR with CNN/HAR_Smartphones.zip\" -d \"/content/drive/My Drive/SP1_2020/HAR with CNN/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ9Ij24xf_fJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard\n",
        "\n",
        "import datetime, os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQF0Hs2qfiPW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "# checkpoint\n",
        "filepath = dataset_path + \"output.best.h5\"\n",
        "checkpoint_callback = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min',save_freq='epoch')\n",
        "\n",
        "# tensorboard\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = TensorBoard(logdir, histogram_freq=1,profile_batch=0)\n",
        "\n",
        "# callbacks\n",
        "callbacks_list = [checkpoint_callback,tensorboard_callback]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pR2KIMAgGmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run Tensorboad for monitoring\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbZXofR7C__X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load a single file as a numpy array\n",
        "def load_file(filepath):\n",
        "    dataframe = pd.read_csv(filepath, header=None, delim_whitespace=True)\n",
        "    return dataframe.values\n",
        "\n",
        "# load a list of files and return as a 3d numpy array\n",
        "def load_group(filenames, prefix=''):\n",
        "    loaded = list()\n",
        "\n",
        "    print(\"\\nShape dimensions:\\n\")\n",
        "    for name in filenames:\n",
        "        data = load_file(prefix + name)\n",
        "        print(np.array(loaded).shape)\n",
        "        loaded.append(data)\n",
        "      \n",
        "     \n",
        "    # stack group so that features are the 3rd dimension\n",
        "    print(np.array(loaded).shape)\n",
        "    loaded = np.dstack(loaded)\n",
        "    print(np.array(loaded).shape)\n",
        "    return loaded\n",
        "\n",
        "# load a dataset group, such as train or test\n",
        "def load_dataset_group(group, prefix=''):\n",
        "    filepath = prefix + group + '/Inertial Signals/'\n",
        "    # load all 9 files as a single array\n",
        "    filenames = list()\n",
        "    # total acceleration\n",
        "    filenames += ['total_acc_x_'+group+'.txt', 'total_acc_y_'+group+'.txt', 'total_acc_z_'+group+'.txt']\n",
        "    # body acceleration\n",
        "    filenames += ['body_acc_x_'+group+'.txt', 'body_acc_y_'+group+'.txt', 'body_acc_z_'+group+'.txt']\n",
        "    # body gyroscope\n",
        "    filenames += ['body_gyro_x_'+group+'.txt', 'body_gyro_y_'+group+'.txt', 'body_gyro_z_'+group+'.txt']\n",
        "    # load input data\n",
        "    X = load_group(filenames, filepath)\n",
        "    # load class output\n",
        "    y = load_file(prefix + group + '/y_'+group+'.txt')\n",
        "    return X, y\n",
        "\n",
        "# load the dataset, returns train and test X and y elements\n",
        "def load_dataset(prefix=''):\n",
        "    # load all train\n",
        "    trainX, trainy = load_dataset_group('train', prefix + 'UCI HAR Dataset/')\n",
        "    # load all test\n",
        "    testX, testy = load_dataset_group('test', prefix + 'UCI HAR Dataset/')\n",
        "    # zero-offset class values\n",
        "    trainy = trainy - 1\n",
        "    testy = testy - 1\n",
        "    print(\"\\nOne Hot Encoding: \\n\",trainy)\n",
        "    # one hot encode y\n",
        "    trainy = to_categorical(trainy)\n",
        "    testy = to_categorical(testy)\n",
        "    print(\"\\n\",trainy)\n",
        "    return trainX, trainy, testX, testy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToE5nuXuCmYy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# fit and evaluate a model\n",
        "def evaluate_model(trainX, trainy, testX, testy):\n",
        "    global model\n",
        "    verbose, epochs, batch_size = 1, 10, 32\n",
        "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
        "    \n",
        "    model.add(LSTM(64, input_shape=(n_timesteps,n_features)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(n_outputs, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    # fit network\n",
        "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,callbacks=callbacks_list)\n",
        "    # evaluate model\n",
        "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
        "    return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4nkoL6CVgtG",
        "colab_type": "code",
        "outputId": "a612261b-986e-43c5-b01d-eab75dfd3695",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        }
      },
      "source": [
        "trainX, trainy, testX, testy = load_dataset(dataset_path)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Shape dimensions:\n",
            "\n",
            "(0,)\n",
            "(1, 7352, 128)\n",
            "(2, 7352, 128)\n",
            "(3, 7352, 128)\n",
            "(4, 7352, 128)\n",
            "(5, 7352, 128)\n",
            "(6, 7352, 128)\n",
            "(7, 7352, 128)\n",
            "(8, 7352, 128)\n",
            "(9, 7352, 128)\n",
            "(7352, 128, 9)\n",
            "\n",
            "Shape dimensions:\n",
            "\n",
            "(0,)\n",
            "(1, 2947, 128)\n",
            "(2, 2947, 128)\n",
            "(3, 2947, 128)\n",
            "(4, 2947, 128)\n",
            "(5, 2947, 128)\n",
            "(6, 2947, 128)\n",
            "(7, 2947, 128)\n",
            "(8, 2947, 128)\n",
            "(9, 2947, 128)\n",
            "(2947, 128, 9)\n",
            "\n",
            "One Hot Encoding: \n",
            " [[4]\n",
            " [4]\n",
            " [4]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n",
            "\n",
            " [[0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]\n",
            " ...\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjDV3qWtVjU4",
        "colab_type": "code",
        "outputId": "250c18b4-e620-4402-918c-d0d683da1b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "score = evaluate_model(trainX, trainy, testX, testy)\n",
        "score = score * 100.0\n",
        "print('>Accuracy: %.3f' % (score))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7352 samples\n",
            "Epoch 1/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.9569 - accuracy: 0.6014\n",
            "Epoch 00001: loss improved from inf to 0.95604, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 17s 2ms/sample - loss: 0.9560 - accuracy: 0.6013\n",
            "Epoch 2/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.5508 - accuracy: 0.7712\n",
            "Epoch 00002: loss improved from 0.95604 to 0.54991, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.5499 - accuracy: 0.7716\n",
            "Epoch 3/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.3411 - accuracy: 0.8735\n",
            "Epoch 00003: loss improved from 0.54991 to 0.34133, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.3413 - accuracy: 0.8734\n",
            "Epoch 4/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1876 - accuracy: 0.9325\n",
            "Epoch 00004: loss improved from 0.34133 to 0.18774, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1877 - accuracy: 0.9324\n",
            "Epoch 5/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1582 - accuracy: 0.9368\n",
            "Epoch 00005: loss improved from 0.18774 to 0.15823, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1582 - accuracy: 0.9368\n",
            "Epoch 6/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1340 - accuracy: 0.9424\n",
            "Epoch 00006: loss improved from 0.15823 to 0.13435, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1343 - accuracy: 0.9422\n",
            "Epoch 7/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1407 - accuracy: 0.9450\n",
            "Epoch 00007: loss did not improve from 0.13435\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1404 - accuracy: 0.9450\n",
            "Epoch 8/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9484\n",
            "Epoch 00008: loss improved from 0.13435 to 0.11914, saving model to /content/drive/My Drive/SP1_2020/HAR with CNN/output.best.h5\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1191 - accuracy: 0.9486\n",
            "Epoch 9/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1200 - accuracy: 0.9509\n",
            "Epoch 00009: loss did not improve from 0.11914\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1205 - accuracy: 0.9505\n",
            "Epoch 10/10\n",
            "7328/7352 [============================>.] - ETA: 0s - loss: 0.1371 - accuracy: 0.9460\n",
            "Epoch 00010: loss did not improve from 0.11914\n",
            "7352/7352 [==============================] - 16s 2ms/sample - loss: 0.1373 - accuracy: 0.9459\n",
            ">Accuracy: 89.108\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJO0de2OWHNU",
        "colab_type": "code",
        "outputId": "d271b196-4e21-4c63-ce99-de878a88dd34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "print(\"\\nsample:\", testX[0])\n",
        "print(\"\\nshape: \", testX[0].shape)\n",
        "print(\"\\nground truth: \", testy[0])\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "sample: [[ 1.041216   -0.2697959   0.02377977 ...  0.4374637   0.5313492\n",
            "   0.1365279 ]\n",
            " [ 1.041803   -0.280025    0.07629271 ...  0.4682641   0.7210685\n",
            "   0.09762239]\n",
            " [ 1.039086   -0.2926631   0.1474754  ...  0.4982574   0.5203284\n",
            "   0.08355578]\n",
            " ...\n",
            " [ 0.9930164  -0.2599865   0.1443951  ... -0.00505586 -0.07734212\n",
            "   0.03225787]\n",
            " [ 0.9932414  -0.2620643   0.1447033  ... -0.02043194 -0.072973\n",
            "   0.02700848]\n",
            " [ 0.9943906  -0.2641348   0.1454939  ... -0.02999741 -0.07064875\n",
            "   0.03054609]]\n",
            "\n",
            "shape:  (128, 9)\n",
            "\n",
            "ground truth:  [0. 0. 0. 0. 1. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3eBPdb3ggzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.load_model(dataset_path+'output.best.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0Ag9rjdXAcS",
        "colab_type": "code",
        "outputId": "e25e2605-05fe-42d9-833e-93f31f888493",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "t = testX[0]\n",
        "t = t.reshape(1,128,9)\n",
        "\n",
        "\n",
        "print(\"\\nclass:\",model.predict_classes(t))\n",
        "print(\"\\nprobs:\",model.predict_proba(t))\n",
        "\n",
        "#print(np.argmax(result))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "class: [4]\n",
            "\n",
            "probs: [[1.4744822e-03 1.4478646e-04 3.9194460e-05 1.9432027e-02 9.7888982e-01\n",
            "  1.9684863e-05]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}